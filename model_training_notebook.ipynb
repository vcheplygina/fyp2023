{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from skimage import io, filters, morphology, segmentation, img_as_ubyte, transform, color\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.draw import polygon\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_segm(img):\n",
    "\n",
    "  img_col = transform.resize(img, (200, 200), anti_aliasing=True)\n",
    "\n",
    "  image = color.rgb2gray(img)\n",
    "  image = transform.resize(image, (200, 200), anti_aliasing=True)\n",
    "  image = filters.gaussian(image)\n",
    "  \n",
    "  # thresholding the image \n",
    "  thresholds = filters.threshold_multiotsu(image, classes=3)\n",
    "  regions = np.digitize(image, thresholds)\n",
    "  output = regions < 1\n",
    "\n",
    "\n",
    "  # making a circle\n",
    "  s = np.linspace(0, 2*np.pi, 100)   #Number of points on the circle\n",
    "  y = 100 + 58*np.sin(s)            #Row \n",
    "  x = 100 + 70*np.cos(s)            #Column\n",
    "  init = np.array([y, x]).T\n",
    "\n",
    "  # and a snake\n",
    "  snake = segmentation.active_contour(output, init, w_line=0)\n",
    "\n",
    "  # Find coordinates inside the polygon defined by the snake\n",
    "  rr, cc = polygon(snake[:, 0], snake[:, 1], output.shape)\n",
    "  mask = np.zeros_like(output)\n",
    "\n",
    "  # applying a mask on the polygon area\n",
    "  mask[rr, cc] = 1\n",
    "  cropped_img2 = output * mask[:, :]\n",
    "\n",
    "  # doing dilation\n",
    "  struct_el = morphology.disk(4)\n",
    "  mask_dilated = morphology.binary_dilation(cropped_img2, struct_el)\n",
    "\n",
    "  # applying the mask and extracting the lesion from the image\n",
    "  im2 = img_col.copy()\n",
    "  im2[mask_dilated == 0] = 0\n",
    "\n",
    "  return im2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Selection and Balancing for Diagnostic Analysis: Creating a Balanced Dataset with Pandas\n",
    "\n",
    "1. The make_df function reads the metadata from a dataset stored in a CSV file called \"metadata.csv\" and creates a pandas DataFrame (df) with the columns \"patient_id\", \"img_id\", and \"diagnostic\". The function then creates a new DataFrame (new_df) by selecting only the columns \"patient_id\", \"img_id\", and \"diagnostic\" from df.\n",
    "2. In the new_df DataFrame, a new column called \"healthy\" is created using the np.where function. The \"healthy\" column is assigned a value of 1 if the corresponding \"diagnostic\" column value is \"NEV\" (indicating a healthy diagnosis), and 0 otherwise.\n",
    "3. The select_data function takes the new_df DataFrame as input. It creates a new DataFrame called final_data by filtering rows where the \"healthy\" column value is 1, indicating a healthy diagnosis.\n",
    "4. Another DataFrame called filtered_data is created by filtering rows where the \"healthy\" column value is 0, indicating a non-healthy diagnosis.\n",
    "5. The sample function is used on the filtered_data DataFrame to randomly select 244 rows (representing non-healthy diagnoses) using a random state of 42. These randomly selected rows are stored in the random_rows DataFrame.\n",
    "6. The pd.concat function is used to concatenate (pd.concat([final_data, random_rows])) the final_data DataFrame and the random_rows DataFrame, resulting in a new DataFrame that combines the healthy and randomly selected non-healthy rows.\n",
    "7. The index of the combined DataFrame is set to \"patient_id\" using the set_index function.\n",
    "8. The sample function is used again on the combined DataFrame with frac=1 to shuffle the rows randomly.\n",
    "9. The resulting shuffled DataFrame is returned as final_data from the select_data function.\n",
    "10 Finally, the final_data DataFrame is printed.\n",
    "\n",
    "The overall purpose of the code is to create a pandas DataFrame (final_data) that represents a selected subset of the original dataset. It ensures that the \"healthy\" diagnosis (\"NEV\") is included in the final DataFrame and randomly selects a certain number of non-healthy diagnoses from the remaining data. The resulting DataFrame is shuffled to provide a random order of the data. This process aims to create a balanced dataset for further analysis or modeling purposes, where both healthy and non-healthy data points are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dubst\\AppData\\Local\\Temp\\ipykernel_23564\\1341983333.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df[\"healthy\"] = np.where(new_df[\"diagnostic\"] == \"NEV\", 1, 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>diagnostic</th>\n",
       "      <th>healthy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PAT_520</th>\n",
       "      <td>PAT_520_983_221.png</td>\n",
       "      <td>BCC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_1934</th>\n",
       "      <td>PAT_1934_3890_306.png</td>\n",
       "      <td>NEV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_244</th>\n",
       "      <td>PAT_244_374_726.png</td>\n",
       "      <td>NEV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_119</th>\n",
       "      <td>PAT_119_181_684.png</td>\n",
       "      <td>BCC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_972</th>\n",
       "      <td>PAT_972_1843_756.png</td>\n",
       "      <td>MEL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_1392</th>\n",
       "      <td>PAT_1392_1352_828.png</td>\n",
       "      <td>SEK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_621</th>\n",
       "      <td>PAT_621_1182_456.png</td>\n",
       "      <td>NEV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_872</th>\n",
       "      <td>PAT_872_1707_638.png</td>\n",
       "      <td>BCC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_1216</th>\n",
       "      <td>PAT_1216_759_365.png</td>\n",
       "      <td>NEV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAT_1947</th>\n",
       "      <td>PAT_1947_3926_165.png</td>\n",
       "      <td>NEV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>488 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           img_id diagnostic  healthy\n",
       "patient_id                                           \n",
       "PAT_520       PAT_520_983_221.png        BCC        0\n",
       "PAT_1934    PAT_1934_3890_306.png        NEV        1\n",
       "PAT_244       PAT_244_374_726.png        NEV        1\n",
       "PAT_119       PAT_119_181_684.png        BCC        0\n",
       "PAT_972      PAT_972_1843_756.png        MEL        0\n",
       "...                           ...        ...      ...\n",
       "PAT_1392    PAT_1392_1352_828.png        SEK        0\n",
       "PAT_621      PAT_621_1182_456.png        NEV        1\n",
       "PAT_872      PAT_872_1707_638.png        BCC        0\n",
       "PAT_1216     PAT_1216_759_365.png        NEV        1\n",
       "PAT_1947    PAT_1947_3926_165.png        NEV        1\n",
       "\n",
       "[488 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_df():\n",
    "  path = os.path.join(os.getcwd(), \"metadata.csv\")\n",
    "  df = pd.read_csv(path)\n",
    "  new_df = df[[\"patient_id\", \"img_id\", \"diagnostic\"]]\n",
    "\n",
    "  new_df[\"healthy\"] = np.where(new_df[\"diagnostic\"] == \"NEV\", 1, 0) \n",
    "  return new_df \n",
    "\n",
    "\n",
    "def select_data(new_df):\n",
    "  final_data = new_df[new_df[\"healthy\"] == 1]\n",
    "  filtered_data = new_df[new_df[\"healthy\"] == 0]\n",
    "\n",
    "  random_rows = filtered_data.sample(n = 244, random_state=42)\n",
    "  final_data = pd.concat([final_data, random_rows])\n",
    "  final_data = final_data.set_index(\"patient_id\")\n",
    "\n",
    "  final_data = final_data.sample(frac=1)\n",
    "\n",
    "  return final_data\n",
    "\n",
    "\n",
    "final_data = select_data(make_df())\n",
    "\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slic_samples(img):\n",
    "  new_img = img.copy()\n",
    "  new_img = new_img[:, :, :3]\n",
    "\n",
    "  foreground_mask = np.all(new_img != [0, 0, 0], axis=-1)\n",
    "\n",
    "  segments = segmentation.slic(new_img * foreground_mask[..., np.newaxis], n_segments=36, compactness=3)\n",
    "\n",
    "  mean_colours = np.zeros((np.max(segments)+1, 3))\n",
    "\n",
    "  for label in enumerate(np.unique(segments)):\n",
    "    mask = segments == label[1]\n",
    "    mean_colours[label[0], :] = new_img[mask].mean(axis=0)\n",
    "\n",
    "  palette_height, palette_width = 50, 300\n",
    "  colours = mean_colours[np.all(mean_colours, axis=1)]\n",
    "  color_palette = np.zeros((palette_height, len(colours), 3))\n",
    "\n",
    "  for i in range(len(colours)):\n",
    "    color_palette[:, i, :] = colours[i]\n",
    "\n",
    "  return color_palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasample(img, name):\n",
    "  # asym = check_asymmetry(img)\n",
    "  col = slic_samples(img)\n",
    "  common_shape = (50, 27, 3)\n",
    "\n",
    "  col = np.pad(col, [(0, common_shape[0] - col.shape[0]),\n",
    "                                  (0, common_shape[1] - col.shape[1]),\n",
    "                                  (0, common_shape[2] - col.shape[2])], mode='constant')\n",
    "\n",
    "  col = col.ravel()\n",
    "  x = col\n",
    "\n",
    "  if (final_data[final_data[\"img_id\"] == name][\"healthy\"] == 1).bool():\n",
    "    y = 1\n",
    "  else:\n",
    "    y = 0\n",
    "  \n",
    "  return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279\n"
     ]
    }
   ],
   "source": [
    "def build_datasample_new():\n",
    "  path = os.path.join(os.getcwd(), \"segmented_photos\")\n",
    "  arr = []\n",
    "\n",
    "  for i in os.listdir(path):\n",
    "    image = io.imread(os.path.join(path, i))\n",
    "    image = transform.resize(image, (200, 200), anti_aliasing=True)\n",
    "\n",
    "    arr.append(make_datasample(image, i))\n",
    "\n",
    "  np.random.shuffle(arr)\n",
    "  return arr\n",
    "\n",
    "\n",
    "arr_col = build_datasample_new()\n",
    "print(len(arr_col))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to calculate asymmetry coefficients by making histograms of a lesion and computing the Bhattacharyya coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_projections(bin_img):\n",
    "  binary_image = bin_img.copy()\n",
    "\n",
    "  vertical = np.sum(binary_image, 0)\n",
    "  horizontal = np.sum(binary_image, 1)\n",
    "\n",
    "  return vertical, horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_correlation(vertical, horizontal):\n",
    "  # Normalize the histograms to ensure they represent probability distributions\n",
    "    hist1_normalized = vertical / np.sum(vertical)\n",
    "    hist2_normalized = horizontal / np.sum(horizontal)\n",
    "\n",
    "    # Compute the Bhattacharyya coefficient\n",
    "    bc = np.sum(np.sqrt(hist1_normalized * hist2_normalized))\n",
    "\n",
    "    # Compute the Bhattacharyya distance\n",
    "    bd = -np.log(bc)\n",
    "\n",
    "    return bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_asymmetry(masked_img):\n",
    "  vert, horiz = make_projections(masked_img.astype(\"double\"))\n",
    "  corr = calc_correlation(vert, horiz)\n",
    "\n",
    "  return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasample_symetry(img, name):\n",
    "  asym = check_asymmetry(img)\n",
    "  x = [asym]\n",
    "\n",
    "  if (final_data[final_data[\"img_id\"] == name][\"healthy\"] == 1).bool():\n",
    "    y = 1\n",
    "  else:\n",
    "    y = 0\n",
    "  \n",
    "  return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279\n"
     ]
    }
   ],
   "source": [
    "def build_datasample_asym():\n",
    "  healthy_dir = os.path.join(os.getcwd(), \"healthy\")\n",
    "  unheatlhy_dir = os.path.join(os.getcwd(), \"unhealthy\")\n",
    "\n",
    "  arr = []\n",
    "\n",
    "  for i in os.listdir(healthy_dir):\n",
    "    image = io.imread(os.path.join(healthy_dir, i))\n",
    "    image = transform.resize(image, (200, 200), anti_aliasing=True)\n",
    "\n",
    "    arr.append(make_datasample_symetry(image, i))\n",
    "  \n",
    "  for i in os.listdir(unheatlhy_dir):\n",
    "    image = io.imread(os.path.join(unheatlhy_dir, i))\n",
    "    image = transform.resize(image, (200, 200), anti_aliasing=True)\n",
    "\n",
    "    arr.append(make_datasample_symetry(image, i))\n",
    "\n",
    "  np.random.shuffle(arr)\n",
    "  return arr\n",
    "\n",
    "arr_cor = build_datasample_asym()\n",
    "print(len(arr_cor))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function **separate_data** for splitting the data into training input samples and target values <br>\n",
    "and an **evaluate** function for getting the metrics' scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(arr):\n",
    "  x = list()\n",
    "  y = list()\n",
    "\n",
    "  for i in arr:\n",
    "    x.append(i[0])\n",
    "    y.append(i[1])\n",
    "\n",
    "  return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "  predictions = model.predict(test_features)\n",
    "  errors = abs(predictions - test_labels)\n",
    "  \n",
    "  accuracy = accuracy_score(test_labels, predictions)\n",
    "  precision = precision_score(test_labels, predictions)\n",
    "  f1_score_res = f1_score(test_labels, predictions)\n",
    "  recall = recall_score(test_labels, predictions)\n",
    "  auc = roc_auc_score(test_labels, predictions)\n",
    "\n",
    "  print(f'Model Performance {type(model).__name__}')\n",
    "  print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "  print('Accuracy = {:0.4f}%.'.format(accuracy*100))\n",
    "  print('Precision = {:0.4f}%.'.format(precision*100))\n",
    "  print('F1 Score = {:0.4f}%.'.format(f1_score_res*100))\n",
    "  print('Recall = {:0.4f}%.'.format(recall*100))\n",
    "  print('AUC = {:0.4f}%.'.format(auc*100))\n",
    "  \n",
    "    \n",
    "  return (np.mean(errors), accuracy, precision, f1_score_res, recall, auc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run final_training function, in order to see, how the different models give output, regarding to such metrics, as: <br> \n",
    "`Average Error`<br> \n",
    "`Accuracy`<br> \n",
    "`Precision`<br> \n",
    "`F1 Score`<br> \n",
    "`Recall`<br> \n",
    "`AUC`<br> \n",
    "`Cross-validation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_training(arr, n_folds, title):\n",
    "\n",
    "  x, y = separate_data(arr)\n",
    "\n",
    "  x = np.array(x)\n",
    "  y = np.array(y)\n",
    "  \n",
    "  (train_col, test_col, train_lab, test_lab) = train_test_split(\n",
    "\tx, y, test_size=0.25, random_state=42) \n",
    "\n",
    "  kf = KFold(n_splits=n_folds)\n",
    "\n",
    "  clf = NearestCentroid()\n",
    "  neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "  rndF = RandomForestClassifier()\n",
    "  classifier = LogisticRegression(max_iter=207,random_state = 0, penalty = None)\n",
    "\n",
    "  result_clf = cross_val_score(clf, train_col, train_lab, cv=n_folds)\n",
    "  result_neigh = cross_val_score(neigh, train_col, train_lab, cv=n_folds)\n",
    "  result_rndF = cross_val_score(rndF, train_col, train_lab, cv=n_folds)\n",
    "  result_classifier = cross_val_score(classifier, train_col, train_lab, cv=n_folds)\n",
    "\n",
    "  clf.fit(train_col, train_lab)\n",
    "  neigh.fit(train_col, train_lab)\n",
    "  rndF.fit(train_col, train_lab)\n",
    "  classifier.fit(train_col, train_lab)\n",
    "  \n",
    "  eval_clf = evaluate(clf, test_col, test_lab); print(f\"Cross-val: {np.mean(result_clf)}\"); print(\"\")\n",
    "  eval_neigh = evaluate(neigh, test_col, test_lab); print(f\"Cross-val: {np.mean(result_neigh)}\"); print(\"\")\n",
    "  eval_rndF = evaluate(rndF, test_col, test_lab); print(f\"Cross-val: {np.mean(result_rndF)}\"); print(\"\")\n",
    "  eval_class = evaluate(classifier, test_col, test_lab); print(f\"Cross-val: {np.mean(result_classifier)}\"); print(\"\")\n",
    "\n",
    "  # Create a list of labels for the models\n",
    "  models = ['NC', 'KNN', 'RFC', 'LR']\n",
    "\n",
    "  # Create a list of metrics for each model\n",
    "  average_errors = [eval_clf[0], eval_neigh[0], eval_rndF[0], eval_class[0]]\n",
    "  accuracies = [eval_clf[1], eval_neigh[1], eval_rndF[1], eval_class[1]]\n",
    "  precisions = [eval_clf[2], eval_neigh[2], eval_rndF[2], eval_class[2]]\n",
    "  f1_scores = [eval_clf[3], eval_neigh[3], eval_rndF[3], eval_class[3]]\n",
    "  recalls = [eval_clf[4], eval_neigh[4], eval_rndF[4], eval_class[4]]\n",
    "  aucs = [eval_clf[5], eval_neigh[5], eval_rndF[5], eval_class[5]]\n",
    "  cross_vals = [np.mean(result_clf), np.mean(result_neigh), np.mean(result_rndF), np.mean(result_classifier)]\n",
    "\n",
    "  # Plotting the histograms\n",
    "  fig, axs = plt.subplots(2, 4, figsize=(12, 5))\n",
    "  axs = axs.flatten()\n",
    "\n",
    "  # Histogram for Average Error\n",
    "  axs[0].bar(models, average_errors)\n",
    "  axs[0].set_title('Average Error')\n",
    "  axs[0].set_ylabel('Degrees')\n",
    "\n",
    "  # Histogram for Accuracy\n",
    "  axs[1].bar(models, accuracies)\n",
    "  axs[1].set_title('Accuracy')\n",
    "  axs[1].set_ylabel('Percentage')\n",
    "\n",
    "  # Histogram for Precision\n",
    "  axs[2].bar(models, precisions)\n",
    "  axs[2].set_title('Precision')\n",
    "  axs[2].set_ylabel('Percentage')\n",
    "\n",
    "  # Histogram for F1 Score\n",
    "  axs[3].bar(models, f1_scores)\n",
    "  axs[3].set_title('F1 Score')\n",
    "  axs[3].set_ylabel('Percentage')\n",
    "\n",
    "  # Histogram for Recall\n",
    "  axs[4].bar(models, recalls)\n",
    "  axs[4].set_title('Recall')\n",
    "  axs[4].set_ylabel('Percentage')\n",
    "\n",
    "  # Histogram for AUC\n",
    "  axs[5].bar(models, aucs)\n",
    "  axs[5].set_title('AUC')\n",
    "  axs[5].set_ylabel('Percentage')\n",
    "\n",
    "  axs[6].bar(models, cross_vals)\n",
    "  axs[6].set_title('Cross-Validation')\n",
    "  axs[6].set_ylabel('Value')\n",
    "\n",
    "  fig.suptitle(title)\n",
    "\n",
    "  return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we define a function called **best_random_forest** to create a model, which is based on the parameters obtained from Grid and Random Searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_random_forest(arr, n_folds, title):\n",
    "  x, y = separate_data(arr)\n",
    "\n",
    "  x = np.array(x)\n",
    "  y = np.array(y)\n",
    "  \n",
    "  (train_col, test_col, train_lab, test_lab) = train_test_split(\n",
    "\tx, y, test_size=0.25, random_state=42) \n",
    "\n",
    "  kf = KFold(n_splits=n_folds)\n",
    "  \n",
    "  rndF = RandomForestClassifier(n_estimators = 1200, min_samples_split=5, min_samples_leaf=3, max_features='sqrt', max_depth=30, bootstrap=True)\n",
    "  result_rndF = cross_val_score(rndF, train_col, train_lab, cv=n_folds)\n",
    "  \n",
    "  rndF.fit(train_col, train_lab)\n",
    "\n",
    "  # joblib.dump(rndF, 'C:\\\\Users\\\\dubst\\\\Desktop\\\\DataScience\\\\Project 2\\\\fyp2023\\\\random_forest_model_cor.pkl')\n",
    "\n",
    "  score_rndF = rndF.score(test_col, test_lab)\n",
    "\n",
    "  eval_rndF = evaluate(rndF, test_col, test_lab)\n",
    "\n",
    "  fig, axs = plt.subplots(2, 4, figsize=(12, 5))\n",
    "  axs = axs.flatten()\n",
    "\n",
    "  # Histogram for Average Error\n",
    "  axs[0].bar(\"Average Error\", eval_rndF[0])\n",
    "  axs[0].set_title(eval_rndF[0])\n",
    "\n",
    "  axs[1].bar(\"Accuracy\", eval_rndF[1])\n",
    "  axs[1].set_title(eval_rndF[1])\n",
    "\n",
    "  axs[2].bar(\"Precision\", eval_rndF[2])\n",
    "  axs[2].set_title(eval_rndF[2])\n",
    "\n",
    "  axs[3].bar(\"F1 Score\", eval_rndF[3])\n",
    "  axs[3].set_title(eval_rndF[3])\n",
    "\n",
    "  axs[4].bar(\"Recall\", eval_rndF[4])\n",
    "  axs[4].set_xlabel(eval_rndF[4])\n",
    "\n",
    "  axs[5].bar(\"AUC\", eval_rndF[5])\n",
    "  axs[5].set_xlabel(eval_rndF[5])\n",
    "\n",
    "  axs[6].bar(\"Cross-Validation\", np.mean(result_rndF))\n",
    "  axs[6].set_xlabel(np.mean(result_rndF))\n",
    "  \n",
    "  fig.suptitle(title)\n",
    "\n",
    "  return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same is for the regular random forest with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_random_forest(arr, n_folds):\n",
    "  x, y = separate_data(arr)\n",
    "\n",
    "  x = np.array(x)\n",
    "  y = np.array(y)\n",
    "  \n",
    "  (train_col, test_col, train_lab, test_lab) = train_test_split(\n",
    "\tx, y, test_size=0.25, random_state=42) \n",
    "\n",
    "  kf = KFold(n_splits=n_folds)\n",
    "  \n",
    "  rndF = RandomForestClassifier()\n",
    "  result_rndF = cross_val_score(rndF, train_col, train_lab, cv=n_folds)\n",
    "  \n",
    "  rndF.fit(train_col, train_lab)\n",
    "\n",
    "  # joblib.dump(rndF, 'C:\\\\Users\\\\dubst\\\\Desktop\\\\DataScience\\\\Project 2\\\\fyp2023\\\\random_forest_model_cor.pkl')\n",
    "\n",
    "  score_rndF = rndF.score(test_col, test_lab)\n",
    "\n",
    "  eval_rndF = evaluate(rndF, test_col, test_lab)\n",
    "\n",
    "  fig, axs = plt.subplots(2, 4, figsize=(12, 5))\n",
    "  axs = axs.flatten()\n",
    "\n",
    "  # Histogram for Average Error\n",
    "  axs[0].bar(\"Average Error\", eval_rndF[0])\n",
    "  axs[0].set_title(eval_rndF[0])\n",
    "\n",
    "  axs[1].bar(\"Accuracy\", eval_rndF[1])\n",
    "  axs[1].set_title(eval_rndF[1])\n",
    "\n",
    "  axs[2].bar(\"Precision\", eval_rndF[2])\n",
    "  axs[2].set_title(eval_rndF[2])\n",
    "\n",
    "  axs[3].bar(\"F1 Score\", eval_rndF[3])\n",
    "  axs[3].set_title(eval_rndF[3])\n",
    "\n",
    "  axs[4].bar(\"Recall\", eval_rndF[4])\n",
    "  axs[4].set_xlabel(eval_rndF[4])\n",
    "\n",
    "  axs[5].bar(\"AUC\", eval_rndF[5])\n",
    "  axs[5].set_xlabel(eval_rndF[5])\n",
    "\n",
    "  axs[6].bar(\"Cross-Validation\", np.mean(result_rndF))\n",
    "  axs[6].set_xlabel(np.mean(result_rndF))\n",
    "  \n",
    "\n",
    "  return fig, eval_rndF, np.mean(result_rndF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we would like to store all the features in a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv_features(path):\n",
    "\n",
    "  healthy_dir = os.path.join(os.getcwd(), \"healthy\")\n",
    "  unhealthy_dir = os.path.join(os.getcwd(), \"unhealthy\")\n",
    "  df = pd.DataFrame([])\n",
    "\n",
    "  for i in os.listdir(healthy_dir):\n",
    "    image = io.imread(os.path.join(healthy_dir, i))[:, :, :3]\n",
    "    image = transform.resize(image, (200, 200), anti_aliasing=True)\n",
    "\n",
    "    colour = make_datasample(image, i)\n",
    "    asym = make_datasample_symetry(image, i)\n",
    "    df[\"img_id\"] = i \n",
    "    df[\"colour\"] = colour[0]\n",
    "\n",
    "    print(colour[0])\n",
    "    df[\"asymmetry coef\"] = asym[0][0]\n",
    "    df[\"healthy\"] = asym[1]\n",
    "\n",
    "  for i in os.listdir(unhealthy_dir):\n",
    "    image = io.imread(os.path.join(unhealthy_dir, i))[:, :, :3]\n",
    "    image = transform.resize(image, (200, 200), anti_aliasing=True)\n",
    "\n",
    "    colour = make_datasample(image, i)\n",
    "    asym = make_datasample_symetry(image, i)\n",
    "    df[\"img_id\"] = i \n",
    "    df[\"colour\"] = colour[0]\n",
    "    df[\"asymmetry coef\"] = asym[0][0]\n",
    "    df[\"healthy\"] = asym[1]\n",
    "\n",
    "  df.to_csv(os.path.join(path, \"output.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the function for comparing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(evals1, evals2, cros1, cros2):\n",
    "  fig, axs = plt.subplots(2, 4, figsize=(12, 5))\n",
    "  axs = axs.flatten()\n",
    "\n",
    "  # Histogram for Average Error\n",
    "  axs[0].bar(\"New\", evals1[0])\n",
    "  axs[0].set_title(\"Average Error\")\n",
    "  axs[0].bar(\"Old\", evals2[0])\n",
    "  axs[0].set_title(\"Average Error\")\n",
    "\n",
    "  axs[1].bar(\"New\", evals1[1])\n",
    "  axs[1].set_title(\"Accuracy\")\n",
    "  axs[1].bar(\"Old\", evals2[1])\n",
    "  axs[1].set_title(\"Accuracy\")\n",
    "\n",
    "  axs[2].bar(\"New\", evals1[2])\n",
    "  axs[2].set_title(\"Precision\")\n",
    "  axs[2].bar(\"Old\", evals2[2])\n",
    "  axs[2].set_title(\"Precision\")\n",
    "\n",
    "  axs[3].bar(\"New\", evals1[3])\n",
    "  axs[3].set_title(\"F1 Score\")\n",
    "  axs[3].bar(\"Old\", evals2[3])\n",
    "  axs[3].set_title(\"F1 Score\")\n",
    "\n",
    "  axs[4].bar(\"New\", evals1[4])\n",
    "  axs[4].set_title(\"Recall\")\n",
    "  axs[4].bar(\"Old\", evals2[4])\n",
    "  axs[4].set_title(\"Recall\")\n",
    "\n",
    "  axs[5].bar(\"New\", evals1[5])\n",
    "  axs[5].set_title(\"AUC\")\n",
    "  axs[5].bar(\"Old\", evals2[5])\n",
    "  axs[5].set_title(\"AUC\")\n",
    "\n",
    "  axs[6].bar(\"New\", cros1)\n",
    "  axs[6].set_title(\"Cross-Validation\")\n",
    "  axs[6].bar(\"Old\", cros2)\n",
    "  axs[6].set_title(\"Cross-Validation\")\n",
    "\n",
    "  return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make tables for the models in terms of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_figures_tables(path):\n",
    "  final_training(arr_col, 10, \"Colour\").show()\n",
    "  final_training(arr_cor, 10, \"Asymmetry\").show()\n",
    "  \n",
    "  best_random_forest(arr_col, 10, \"Random Forest Colour\").show()\n",
    "  best_random_forest(arr_cor, 10, \"Random Forest Asymmetry\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_figures_tables(os.path.join(os.getcwd(), \"output.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, there are functions, which are meant for predicting the state of a lesion in relation to masks and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask_col(img):\n",
    "  rndF = joblib.load(os.path.join(os.getcwd(), \"random_forest_model_col.pkl\"))\n",
    "  image = transform.resize(img, (200, 200), anti_aliasing=True)\n",
    "\n",
    "  col = slic_samples(image)\n",
    "  common_shape = (50, 27, 3)\n",
    "\n",
    "  col = np.pad(col, [(0, common_shape[0] - col.shape[0]),\n",
    "                                  (0, common_shape[1] - col.shape[1]),\n",
    "                                  (0, common_shape[2] - col.shape[2])], mode='constant')\n",
    "\n",
    "  col = col.ravel()\n",
    "  print(col)\n",
    "  result = rndF.predict([col])\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask_cor(img):\n",
    "  rndF = joblib.load(os.path.join(os.getcwd(), \"random_forest_model_cor.pkl\"))\n",
    "  image = transform.resize(img, (200, 200), anti_aliasing=True)\n",
    "  asym = check_asymmetry(image)\n",
    "  asym = [asym]\n",
    "\n",
    "  result = rndF.predict(np.array([asym]))\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img_col(img):\n",
    "  rndF = joblib.load(os.path.join(os.getcwd(), \"random_forest_model_col.pkl\"))\n",
    "  img = img[:, :, :3]\n",
    "  image = do_segm(img)\n",
    "  plt.imshow(image)\n",
    "  plt.show()\n",
    "  col = slic_samples(image)\n",
    "  common_shape = (50, 27, 3)\n",
    "\n",
    "  col = np.pad(col, [(0, common_shape[0] - col.shape[0]),\n",
    "                                  (0, common_shape[1] - col.shape[1]),\n",
    "                                  (0, common_shape[2] - col.shape[2])], mode='constant')\n",
    "\n",
    "  col = col.ravel()\n",
    "  result = rndF.predict([col])\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img_cor(img):\n",
    "  rndF = joblib.load(os.path.join(os.getcwd(), \"random_forest_model_cor.pkl\"))\n",
    "  image = do_segm(img)\n",
    "  asym = check_asymmetry(image)\n",
    "  asym = [asym]\n",
    "\n",
    "  result = rndF.predict(np.array([asym]))\n",
    "\n",
    "  return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
